{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graded_Assignment_5_Date_translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnvvS5l0+u+9mQ0ercnTCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Poohi5/cmpe258_homework/blob/master/Graded_Assignment_5/Graded_Assignment_5_Date_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YbUZOstOxxD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "5c73b5de-aa0c-4570-c9b8-dac8e199262a"
      },
      "source": [
        "!pip install Faker"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Faker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/9d/39cc13537ba25a7819bd566d0b0cddfa5570579c194756ac3aff57256dcd/Faker-4.1.0-py3-none-any.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 5.9MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 7.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 7.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 8.5MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 8.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 8.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 8.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▏                           | 133kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 143kB 8.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 153kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 163kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 174kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 184kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 194kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 204kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 215kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 235kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 245kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 256kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 266kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 276kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 286kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 296kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 307kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 317kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 327kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 337kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 348kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 358kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 368kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 378kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 389kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 399kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 409kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 419kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 430kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 440kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 450kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 460kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 471kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 481kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 491kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 501kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 512kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 522kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 532kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 542kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 552kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 563kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 573kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 583kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 593kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 604kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 614kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 624kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 634kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 645kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 655kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 665kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 675kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 686kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 696kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 706kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 716kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 727kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 737kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 747kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 757kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 768kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 778kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 788kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 798kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 808kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 819kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 829kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 839kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 849kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 860kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 870kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 880kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 890kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 901kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 911kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 921kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 931kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 942kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 952kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 962kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 972kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 983kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 993kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.0MB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from Faker) (2.8.1)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from Faker) (1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.4->Faker) (1.12.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "246-GIg8OsFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "97e4311d-e05d-4aa2-d7e7-a8d83fed8bf7"
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "# from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0k3vkmaOzNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fake = Faker()\n",
        "# fake.seed(12345)\n",
        "random.seed(12345)\n",
        "\n",
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']\n",
        "\n",
        "# change this if you want it to work with another language\n",
        "LOCALES = ['en_US']\n",
        "\n",
        "def load_date():\n",
        "    \"\"\"\n",
        "        Loads some fake dates \n",
        "        :returns: tuple containing human readable string, machine readable string, and date object\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "        \n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt\n",
        "\n",
        "def load_dataset(m):\n",
        "    \"\"\"\n",
        "        Loads a dataset with m examples and vocabularies\n",
        "        :m: the number of examples to generate\n",
        "    \"\"\"\n",
        "    \n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    Tx = 30\n",
        "    \n",
        "\n",
        "    for i in tqdm(range(m)):\n",
        "        h, m, _ = load_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, m))\n",
        "            human_vocab.update(tuple(h))\n",
        "            machine_vocab.update(tuple(m))\n",
        "    \n",
        "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
        "                     list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
        "    machine = {v:k for k,v in inv_machine.items()}\n",
        " \n",
        "    return dataset, human, machine, inv_machine\n",
        "\n",
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
        "    \n",
        "    X, Y = zip(*dataset)\n",
        "    \n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
        "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
        "    \n",
        "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "    return X, np.array(Y), Xoh, Yoh\n",
        "\n",
        "def string_to_int(string, length, vocab):\n",
        "    \"\"\"\n",
        "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
        "    input string's characters in the \"vocab\"\n",
        "    \n",
        "    Arguments:\n",
        "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
        "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
        "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
        "    \n",
        "    Returns:\n",
        "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
        "    \"\"\"\n",
        "    \n",
        "    #make lower to standardize\n",
        "    string = string.lower()\n",
        "    string = string.replace(',','')\n",
        "    \n",
        "    if len(string) > length:\n",
        "        string = string[:length]\n",
        "        \n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
        "    \n",
        "    if len(string) < length:\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\n",
        "    \n",
        "    #print (rep)\n",
        "    return rep\n",
        "\n",
        "\n",
        "def int_to_string(ints, inv_vocab):\n",
        "    \"\"\"\n",
        "    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n",
        "    \n",
        "    Arguments:\n",
        "    ints -- list of integers representing indexes in the machine's vocabulary\n",
        "    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n",
        "    \n",
        "    Returns:\n",
        "    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n",
        "    \"\"\"\n",
        "    \n",
        "    l = [inv_vocab[i] for i in ints]\n",
        "    return l\n",
        "\n",
        "\n",
        "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
        "\n",
        "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
        "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
        "    prediction = model.predict(np.array([encoded]))\n",
        "    prediction = np.argmax(prediction[0], axis=-1)\n",
        "    return int_to_string(prediction, inv_output_vocabulary)\n",
        "\n",
        "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
        "    predicted = []\n",
        "    for example in examples:\n",
        "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
        "        print('input:', example)\n",
        "        print('output:', predicted[-1])\n",
        "    return predicted\n",
        "\n",
        "\n",
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
        "        \n",
        "\n",
        "def plot_attention_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 6, Tx = 30, Ty = 10):\n",
        "    \"\"\"\n",
        "    Plot the attention map.\n",
        "  \n",
        "    \"\"\"\n",
        "    attention_map = np.zeros((10, 30))\n",
        "    Ty, Tx = attention_map.shape\n",
        "    \n",
        "    s0 = np.zeros((1, n_s))\n",
        "    c0 = np.zeros((1, n_s))\n",
        "    layer = model.layers[num]\n",
        "\n",
        "    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n",
        "    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
        "\n",
        "    f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
        "    r = f([encoded, s0, c0])\n",
        "    \n",
        "    for t in range(Ty):\n",
        "        for t_prime in range(Tx):\n",
        "            attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
        "\n",
        "    # Normalize attention map\n",
        "#     row_max = attention_map.max(axis=1)\n",
        "#     attention_map = attention_map / row_max[:, None]\n",
        "\n",
        "    prediction = model.predict([encoded, s0, c0])\n",
        "    \n",
        "    predicted_text = []\n",
        "    for i in range(len(prediction)):\n",
        "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
        "        \n",
        "    predicted_text = list(predicted_text)\n",
        "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
        "    text_ = list(text)\n",
        "    \n",
        "    # get the lengths of the string\n",
        "    input_length = len(text)\n",
        "    output_length = Ty\n",
        "    \n",
        "    # Plot the attention_map\n",
        "    plt.clf()\n",
        "    f = plt.figure(figsize=(8, 8.5))\n",
        "    ax = f.add_subplot(1, 1, 1)\n",
        "\n",
        "    # add image\n",
        "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "    # add colorbar\n",
        "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
        "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
        "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
        "\n",
        "    # add labels\n",
        "    ax.set_yticks(range(output_length))\n",
        "    ax.set_yticklabels(predicted_text[:output_length])\n",
        "\n",
        "    ax.set_xticks(range(input_length))\n",
        "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
        "\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "\n",
        "    # add grid and legend\n",
        "    ax.grid()\n",
        "\n",
        "    #f.show()\n",
        "    \n",
        "    return attention_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYC22j4cPEbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3ba2209-9ba1-4ce6-d35f-35512bf1df33"
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 18864.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg9IBgGQPGE-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "52ae1c80-07c0-4065-a7ec-f0a312dbaaa7"
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9o47LGsPJBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "485eabfd-8432-4616-e9e5-33b728d988ef"
      },
      "source": [
        "index = 0\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source date: 19 feb 1980\n",
            "Target date: 1980-02-19\n",
            "\n",
            "Source after preprocessing (indices): [ 4 12  0 18 17 14  0  4 12 11  3 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 2 10  9  1  0  1  3  0  2 10]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivNT-qSAPLDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQB29IGwPNOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "    \n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "    \n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = repeator(s_prev)\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    concat = concatenator([a, s_prev])\n",
        "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e. (≈1 lines)\n",
        "    e = densor1(concat)\n",
        "    e = densor2(e)\n",
        "    # Use activator and e to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = activator(e)\n",
        "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas, a])\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QShoc5BIPO5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_a = 32\n",
        "n_s = 64\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0RdU94lPQUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    \n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
        "    \n",
        "    # Step 2: Iterate \n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a, s)\n",
        "        \n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
        "        \n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(s)\n",
        "        \n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "    \n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvdHB2lrPS2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_788kLPUSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out = model.compile(optimizer=Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01),\n",
        "                    metrics=['accuracy'],\n",
        "                    loss='categorical_crossentropy')\n",
        "out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-EP72BqPXpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f69-PYwoPZJZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "92886672-f60b-4885-a204-2e976f7f93a3"
      },
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "10000/10000 [==============================] - 21s 2ms/step - loss: 16.5082 - dense_3_loss: 2.5615 - dense_3_accuracy: 0.4505 - dense_3_accuracy_1: 0.6912 - dense_3_accuracy_2: 0.3261 - dense_3_accuracy_3: 0.0895 - dense_3_accuracy_4: 0.9458 - dense_3_accuracy_5: 0.3627 - dense_3_accuracy_6: 0.0563 - dense_3_accuracy_7: 0.9446 - dense_3_accuracy_8: 0.2349 - dense_3_accuracy_9: 0.1036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f13f758c6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nva8FzVPPZmw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3abc4dcd-b9b0-41bb-ff9e-1a41b024151f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
            "                                                                 dense_1[1][0]                    \n",
            "                                                                 dense_1[2][0]                    \n",
            "                                                                 dense_1[3][0]                    \n",
            "                                                                 dense_1[4][0]                    \n",
            "                                                                 dense_1[5][0]                    \n",
            "                                                                 dense_1[6][0]                    \n",
            "                                                                 dense_1[7][0]                    \n",
            "                                                                 dense_1[8][0]                    \n",
            "                                                                 dense_1[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
            "                                                                 dense_2[1][0]                    \n",
            "                                                                 dense_2[2][0]                    \n",
            "                                                                 dense_2[3][0]                    \n",
            "                                                                 dense_2[4][0]                    \n",
            "                                                                 dense_2[5][0]                    \n",
            "                                                                 dense_2[6][0]                    \n",
            "                                                                 dense_2[7][0]                    \n",
            "                                                                 dense_2[8][0]                    \n",
            "                                                                 dense_2[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 (None, 64)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "                                                                 lstm_1[2][0]                     \n",
            "                                                                 lstm_1[3][0]                     \n",
            "                                                                 lstm_1[4][0]                     \n",
            "                                                                 lstm_1[5][0]                     \n",
            "                                                                 lstm_1[6][0]                     \n",
            "                                                                 lstm_1[7][0]                     \n",
            "                                                                 lstm_1[8][0]                     \n",
            "                                                                 lstm_1[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eq81nZaPbBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "07e22656-13b3-49c8-839c-296d5e381f64"
      },
      "source": [
        "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x612 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-519IZM1Pq3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}